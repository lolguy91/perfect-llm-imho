# The perfect LLM(Concept)
The idea to create the perfect LLM currently possible came to my mind because I was watching a YouTube on GaLore, the "sequel" to LoRa, and I realized how fucking groundbreaking that tech is. I was daydreaming about pretraining my own model, this (probably impossible to implement) concept is a refined version of that model.
## A rough sketch
it would have the following features:
- 1 or 2 billion parameters, lightweight for running on CPU
- Mixture Of Eperts architecture
- Support for 8 modalities/formats: text, image ,GIF ,binary blob ,PDF ,EPUB ,audio ,MIDI
- Trained on heavily filtered, yet unbiased and high quality data(only filtering out garbage like text from web apps)
- Uncensored(!!!)
- Quiet StAR support
- Beeg context window(at least 512k)
